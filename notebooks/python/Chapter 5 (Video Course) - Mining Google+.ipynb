{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining the Social Web\n",
    "\n",
    "## Mining Text Files\n",
    "\n",
    "This Jupyter Notebook provides an interactive way to follow along with and explore the examples from the video series. The intent behind this notebook is to reinforce the concepts in a fun, convenient, and effective way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for a person with the Google+ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib2\n",
    "import json\n",
    "import apiclient.discovery # pip install google-api-python-client\n",
    "\n",
    "# See https://developers.google.com/+/web/api/rest/\n",
    "\n",
    "# XXX: Enter in your API key from https://console.developers.google.com/apis/credentials\n",
    "API_KEY = '' \n",
    "\n",
    "# XXX: Enter any person's name\n",
    "Q = \"Tim O'Reilly\"\n",
    "\n",
    "service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), \n",
    "                                    developerKey=API_KEY)\n",
    "\n",
    "people_feed = service.people().search(query=Q).execute()\n",
    "\n",
    "print(json.dumps(people_feed['items'], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Google+ avatars in IPython Notebook provides a quick way to disambiguate the search results and discover the person you are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "html = []\n",
    "\n",
    "for p in people_feed['items']:\n",
    "    html += ['<p><img src=\"{}\" /> {}: {}</p>'.format(\n",
    "             p['image']['url'], p['id'], p['displayName'])]\n",
    "\n",
    "HTML(''.join(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching recent activities for a particular Google+ user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib2\n",
    "import json\n",
    "import apiclient.discovery\n",
    "\n",
    "USER_ID = '107033731246200681024' # Tim O'Reilly\n",
    "\n",
    "service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), \n",
    "                                    developerKey=API_KEY)\n",
    "\n",
    "activity_feed = service.activities().list(\n",
    "  userId=USER_ID,\n",
    "  collection='public',\n",
    "  maxResults='100' # Max allowed per API\n",
    ").execute()\n",
    "\n",
    "print(json.dumps(activity_feed, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning HTML in Google+ content by stripping out HTML tags and converting HTML entities back to plain-text representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # pip install beautifulsoup4\n",
    "\n",
    "def cleanHtml(html):\n",
    "    if html == \"\": return \"\"\n",
    "\n",
    "    return BeautifulSoup(html, 'html5lib').get_text().replace(u'\\ufeff', '')\n",
    "\n",
    "\n",
    "print(activity_feed['items'][0]['object']['content'])\n",
    "print()\n",
    "print(cleanHtml(activity_feed['items'][0]['object']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over multiple pages of Google+ activities and distilling clean text from notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import httplib2\n",
    "import json\n",
    "import apiclient.discovery\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "USER_ID = '107033731246200681024' # Tim O'Reilly\n",
    "\n",
    "MAX_RESULTS = 200 # Will require multiple requests\n",
    "\n",
    "\n",
    "def cleanHtml(html):\n",
    "    if html == \"\": return \"\"\n",
    "\n",
    "    return BeautifulSoup(html, 'html5lib').get_text().replace(u'\\ufeff', '')\n",
    "\n",
    "\n",
    "service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), \n",
    "                                    developerKey=API_KEY)\n",
    "\n",
    "activity_feed = service.activities().list(\n",
    "  userId=USER_ID,\n",
    "  collection='public',\n",
    "  maxResults='100' # Max allowed per request\n",
    ")\n",
    "\n",
    "activity_results = []\n",
    "\n",
    "while activity_feed != None and len(activity_results) < MAX_RESULTS:\n",
    "    activities = activity_feed.execute()\n",
    "    if 'items' in activities:\n",
    "        for activity in activities['items']:\n",
    "            if activity['object']['objectType'] == 'note' and activity['object']['content'] != '':\n",
    "                activity['title'] = cleanHtml(activity['title'])\n",
    "                activity['object']['content'] = cleanHtml(activity['object']['content'])\n",
    "                activity_results += [activity]\n",
    "\n",
    "    # list_next requires the previous request and response objects\n",
    "    activity_feed = service.activities().list_next(activity_feed, activities)\n",
    "\n",
    "# Write the output to a file for convenience\n",
    "f = open(os.path.join(USER_ID + '.json'), 'w+')\n",
    "f.write(json.dumps(activity_results, indent=1))\n",
    "f.close()\n",
    "\n",
    "print(str(len(activity_results)), \"activities written to\", f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data structures used in illustrations for the rest of this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = { \n",
    " 'a' : \"Mr. Green killed Colonel Mustard in the study with the candlestick. \\\n",
    "Mr. Green is not a very nice fellow.\",\n",
    " 'b' : \"Professor Plum has a green plant in his study.\",\n",
    " 'c' : \"Miss Scarlett watered Professor Plum's green plant while he was away \\\n",
    "from his office last week.\"\n",
    "}\n",
    "terms = {\n",
    " 'a' : [ i.lower() for i in corpus['a'].split() ],\n",
    " 'b' : [ i.lower() for i in corpus['b'].split() ],\n",
    " 'c' : [ i.lower() for i in corpus['c'].split() ]\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TF-IDF on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(term, doc, normalize=True):\n",
    "    doc = doc.lower().split()\n",
    "    if normalize:\n",
    "        return doc.count(term.lower()) / float(len(doc))\n",
    "    else:\n",
    "        return doc.count(term.lower()) / 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(term, corpus):\n",
    "    num_texts_with_term = len([True for text in corpus if term.lower()\n",
    "                              in text.lower().split()])\n",
    "\n",
    "    # tf-idf calc involves multiplying against a tf value less than 0, so it's\n",
    "    # necessary to return a value greater than 1 for consistent scoring. \n",
    "    # (Multiplying two values less than 1 returns a value less than each of \n",
    "    # them.)\n",
    "\n",
    "    try:\n",
    "        return 1.0 + log(float(len(corpus)) / num_texts_with_term)\n",
    "    except ZeroDivisionError:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(term, doc, corpus):\n",
    "    return tf(term, doc) * idf(term, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# XXX: Enter in a query term from the corpus variable\n",
    "QUERY_TERMS = ['mr.', 'green']\n",
    "\n",
    "\n",
    "for (k, v) in sorted(corpus.items()):\n",
    "    print(k, ':', v)\n",
    "print()\n",
    "    \n",
    "# Score queries by calculating cumulative tf_idf score for each term in query\n",
    "\n",
    "query_scores = {'a': 0, 'b': 0, 'c': 0}\n",
    "for term in [t.lower() for t in QUERY_TERMS]:\n",
    "    for doc in sorted(corpus):\n",
    "        print('TF({0}): {1}'.format(doc, term), tf(term, corpus[doc]))\n",
    "    print('IDF: {0}'.format(term), idf(term, corpus.values()))\n",
    "    print()\n",
    "\n",
    "    for doc in sorted(corpus):\n",
    "        score = tf_idf(term, corpus[doc], corpus.values())\n",
    "        print('TF-IDF({0}): {1}'.format(doc, term), score)\n",
    "        query_scores[doc] += score\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Overall TF-IDF scores for query '{0}'\".format(' '.join(QUERY_TERMS)))\n",
    "for (doc, score) in sorted(query_scores.items()):\n",
    "    print(doc, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Google+ data with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explore some of NLTK's functionality by exploring the data. \n",
    "# Here are some suggestions for an interactive interpreter session.\n",
    "\n",
    "import nltk # pip install nltk\n",
    "\n",
    "# Download ancillary nltk packages if not already installed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load in human language data from wherever you've saved it\n",
    "DATA = '107033731246200681024.json'\n",
    "data = json.loads(open(DATA).read())\n",
    "\n",
    "all_content = \" \".join([ a['object']['content'] for a in data ])\n",
    "\n",
    "# Approximate bytes of text\n",
    "print(len(all_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = all_content.split()\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examples of the appearance of the word \"open\"\n",
    "text.concordance(\"open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequent collocations in the text (usually meaningful phrases)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency analysis for words of interest\n",
    "fdist = text.vocab()\n",
    "print(fdist[\"open\"])\n",
    "print(fdist[\"source\"])\n",
    "print(fdist[\"web\"])\n",
    "print(fdist[\"2.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of words in the text\n",
    "print('Number of tokens:', len(tokens))\n",
    "print()\n",
    "\n",
    "# Number of unique words in the text\n",
    "print('Number of unique words:', len(fdist.keys()))\n",
    "print()\n",
    "\n",
    "# Common words that aren't stopwords\n",
    "print('Common words that aren\\'t stopwords')\n",
    "print([w for w in list(fdist.keys())[:100] \\\n",
    "   if w.lower() not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "print()\n",
    "\n",
    "# Long words that aren't URLs\n",
    "print('Long words that aren\\'t URLs')\n",
    "print([w for w in fdist.keys() if len(w) > 15 and 'http' not in w])\n",
    "print()\n",
    "\n",
    "# Number of URLs\n",
    "print('Number of URLs: ',len([w for w in fdist.keys() if 'http' in w]))\n",
    "print()\n",
    "\n",
    "# Top 10 Most Common Words\n",
    "print('Top 10 Most Common Words')\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Google+ data with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# XXX: Provide your own query terms here\n",
    "\n",
    "QUERY_TERMS = ['Government']\n",
    "\n",
    "activities = [activity['object']['content'].lower().split() \\\n",
    "              for activity in data \\\n",
    "                if activity['object']['content'] != \"\"]\n",
    "\n",
    "# TextCollection provides tf, idf, and tf_idf abstractions so \n",
    "# that we don't have to maintain/compute them ourselves\n",
    "\n",
    "tc = nltk.TextCollection(activities)\n",
    "\n",
    "relevant_activities = []\n",
    "\n",
    "for idx in range(len(activities)):\n",
    "    score = 0\n",
    "    for term in [t.lower() for t in QUERY_TERMS]:\n",
    "        score += tc.tf_idf(term, activities[idx])\n",
    "    if score > 0:\n",
    "        relevant_activities.append({'score': score, 'title': data[idx]['title'],\n",
    "                              'url': data[idx]['url']})\n",
    "\n",
    "# Sort by score and display results\n",
    "\n",
    "relevant_activities = sorted(relevant_activities, \n",
    "                             key=lambda p: p['score'], reverse=True)\n",
    "for activity in relevant_activities:\n",
    "    print(activity['title'])\n",
    "    print('\\tLink: {0}'.format(activity['url']))\n",
    "    print('\\tScore: {0}'.format(activity['score']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar documents using cosine similarity\n",
    "\n",
    "The dot product of two vectors **A** and **B** can be thought of as a projection of one vector into the other.\n",
    "<img src=\"dot_product.png\">\n",
    "\n",
    "By measuring how much of **A** is in the same direction as **B**, we get a measure of how similar **A** is to **B**. The idea behind the following exercise is to create vectors for each document in our corpus consisting of the TF-IDF scores of the terms in those documents:\n",
    "\n",
    "```\n",
    "v_1 = [ tf_idf(term_1, doc_1), tf_idf(term_2, doc_1), ..., tf_idf(term_n, doc_1) ]\n",
    "v_2 = [ tf_idf(term_1, doc_2), tf_idf(term_2, doc_2), ..., tf_idf(term_n, doc_2) ]\n",
    "```\n",
    "\n",
    "The dot product of these vectors:\n",
    "\n",
    "$\\mathbf{v_1} \\cdot \\mathbf{v_2} = |\\mathbf{v_1}||\\mathbf{v_2}|\\cos(\\theta)$.\n",
    "\n",
    "Now you see where the cosine comes in. The \"cosine distance\" between $\\mathbf{v1}$ and $\\mathbf{v2}$ is then given by\n",
    "\n",
    "$$\n",
    "d = 1 - \\frac{\\mathbf{v_1} \\cdot \\mathbf{v_2}}{|\\mathbf{v_1}||\\mathbf{v_2}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# Load in human language data from wherever you've saved it\n",
    "DATA = '107033731246200681024.json'\n",
    "data = json.loads(open(DATA).read())\n",
    "\n",
    "# Only consider content that's ~1000+ chars.\n",
    "data = [ post for post in json.loads(open(DATA).read())\n",
    "         if len(post['object']['content']) > 1000 ]\n",
    "\n",
    "all_posts = [post['object']['content'].lower().split() \n",
    "             for post in data ]\n",
    "\n",
    "\n",
    "# Provides tf, idf, and tf_idf abstractions for scoring\n",
    "tc = nltk.TextCollection(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute a term-document matrix such that td_matrix[doc_title][term]\n",
    "# returns a tf-idf score for the term in the document\n",
    "td_matrix = {}\n",
    "for idx in range(len(all_posts)):\n",
    "    post = all_posts[idx]\n",
    "    fdist = nltk.FreqDist(post)\n",
    "\n",
    "    doc_title = data[idx]['title']\n",
    "    url = data[idx]['url']\n",
    "    td_matrix[(doc_title, url)] = {}\n",
    "\n",
    "    for term in fdist.keys():\n",
    "        td_matrix[(doc_title, url)][term] = tc.tf_idf(term, post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vectors such that term scores are in the same positions...\n",
    "distances = {}\n",
    "for (title1, url1) in td_matrix.keys():\n",
    "\n",
    "    distances[(title1, url1)] = {}\n",
    "    (min_dist, most_similar) = (1.0, ('', ''))\n",
    "\n",
    "    for (title2, url2) in td_matrix.keys():\n",
    "\n",
    "        # Take care not to mutate the original data structures\n",
    "        # since we're in a loop and need the originals multiple times\n",
    "\n",
    "        terms1 = td_matrix[(title1, url1)].copy()\n",
    "        terms2 = td_matrix[(title2, url2)].copy()\n",
    "\n",
    "        # Fill in \"gaps\" in each map so vectors of the same length can be computed\n",
    "        for term1 in terms1:\n",
    "            if term1 not in terms2:\n",
    "                terms2[term1] = 0\n",
    "\n",
    "        for term2 in terms2:\n",
    "            if term2 not in terms1:\n",
    "                terms1[term2] = 0\n",
    "\n",
    "        # Create vectors from term maps\n",
    "        v1 = [score for (term, score) in sorted(terms1.items())]\n",
    "        v2 = [score for (term, score) in sorted(terms2.items())]\n",
    "\n",
    "        # Compute similarity amongst documents\n",
    "        distances[(title1, url1)][(title2, url2)] = \\\n",
    "            nltk.cluster.util.cosine_distance(v1, v2)\n",
    "\n",
    "        if url1 == url2:\n",
    "            #print distances[(title1, url1)][(title2, url2)]\n",
    "            continue\n",
    "\n",
    "        if distances[(title1, url1)][(title2, url2)] < min_dist:\n",
    "            (min_dist, most_similar) = (distances[(title1, url1)][(title2,\n",
    "                                         url2)], (title2, url2))\n",
    "    \n",
    "    print('''Most similar (score: {0:6.4f}) to \n",
    "    \\t\"{1:.40}...\"\\t({2})\n",
    "    \\t\"{3:.40}...\"\\t({4})\n",
    "    '''.format(1-min_dist, title1.replace('\\n',''), url1, \n",
    "               most_similar[0].replace('\\n',''), most_similar[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the article titles - the keys to the 'distances' dict\n",
    "keys = list(distances.keys())\n",
    "\n",
    "# Extract the article titles\n",
    "titles = [l[0][:40].replace('\\n',' ')+'...' for l in list(distances.keys())]\n",
    "n_articles = len(titles)\n",
    "\n",
    "# Initialize the matrix of appropriate size to store similarity scores\n",
    "similarity_matrix = np.zeros((n_articles, n_articles))\n",
    "\n",
    "# Loop over the cells in the matrix\n",
    "for i in range(n_articles):\n",
    "    for j in range(n_articles):\n",
    "        # Retrieve the cosine distance between articles i and j\n",
    "        d = distances[keys[i]][keys[j]]\n",
    "        \n",
    "        # Store the 'similarity' between articles i and j, defined as 1.0 - distance\n",
    "        similarity_matrix[i, j] = 1.0 - d\n",
    "\n",
    "\n",
    "# Create a figure and axes\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Visualize the matrix with colored squares indicating similarity\n",
    "ax.matshow(similarity_matrix, cmap='Greys', vmin = 0.0, vmax = 0.2)\n",
    "\n",
    "# Set regular ticks, one for each article in the collection\n",
    "ax.set_xticks(range(n_articles))\n",
    "ax.set_yticks(range(n_articles))\n",
    "\n",
    "# Set the tick labels as the article titles\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(titles)\n",
    "\n",
    "# Rotate the labels on the x-axis by 90 degrees\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using NLTK to compute bigrams and collocations for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Mr. Green killed Colonel Mustard in the study with the \" + \\\n",
    "           \"candlestick. Mr. Green is not a very nice fellow.\"\n",
    "\n",
    "print([bg for bg in nltk.ngrams(sentence.split(), 2)])\n",
    "txt = nltk.Text(sentence.split())\n",
    "\n",
    "txt.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK to compute collocations in a similar manner to the nltk.Text.collocations demo functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.metrics import association\n",
    "\n",
    "# Load in human language data from wherever you've saved it\n",
    "\n",
    "DATA = 'resources/ch04-googleplus/107033731246200681024.json'\n",
    "data = json.loads(open(DATA).read())\n",
    "\n",
    "# Number of collocations to find\n",
    "\n",
    "N = 25\n",
    "\n",
    "all_tokens = [token for activity in data for token in activity['object']['content'\n",
    "              ].lower().split()]\n",
    "\n",
    "finder = nltk.BigramCollocationFinder.from_words(all_tokens)\n",
    "finder.apply_freq_filter(2)\n",
    "finder.apply_word_filter(lambda w: w in nltk.corpus.stopwords.words('english'))\n",
    "scorer = association.BigramAssocMeasures.jaccard\n",
    "collocations = finder.nbest(scorer, N)\n",
    "\n",
    "for collocation in collocations:\n",
    "    c = ' '.join(collocation)\n",
    "    print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
