{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sentimentpost.png\">,\n",
    "\n",
    "## Applying Sentiment analysis to movie review using Kaggle labeled dataset.\n",
    "\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Kaggle dataset to access from a Postgres DB\n",
    "\n",
    "\n",
    "## Part 1- Write data to Postgres DB   \n",
    "__Objective__:  \n",
    "\n",
    "The purpose of this post besides creating and measuring the accuracy of a sentiment model to predict movie reviews is to practice most of the processes necessary to implement a working Docker environment.  From what I witnessed, many of the cool data mining and analytics I have uncovered are most useful when the results are easily accessed in a database.  This example uses Postgres database in a contained Docker-Compose environment.  Most of the essential parts of extracting, transforming, and loading (ETL) are acccomplished in this post.  \n",
    "This sentiment model example uses the kaggle sample att he link above. My original sources for learning the basics of NLP included using the concepts learned in _Python 3 Text Processing with NLTK 3 Cookbook_ and _pythonprogramming.net_. In practice, I have used the concepts learned here to implement a live sentiment system for customer and support calls stored in salesforce.com. Although part 1 could have been done using Pandas, I prefer to use Postges in a Docker service to make the data persistent and allow for easily querying the data using Adminer.   \n",
    "\n",
    "\n",
    "1.  import csv files downloaded from kaggle\n",
    "2.  load data to a Postgres db in docker container  \n",
    "3.  clean train and test tables to adapt Kaggle dataset to reflect more likely original data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/bin/python3.6\n",
    "#import logging\n",
    "import os\n",
    "import pyodbc\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postgres authentication\n",
    "user = \"alexp\"\n",
    "password = \"secret\"\n",
    "host = \"pg_db\"\n",
    "port = \"5432\"\n",
    "database = \"priv_workspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  import csv files downloaded from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view file names from kaggle unzipped downloads contents:\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 10 lines\n",
    "!head -2 train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned up CSV versions of files with consistent row lengths.\n",
    "# Postgres COPY doesn't handle \"ragged\" files very well\n",
    "\n",
    "import csv\n",
    "\n",
    "txt_file = 'train.tsv'\n",
    "csv_file =  'train.csv'\n",
    "\n",
    "with open(txt_file, 'r') as txt_file:\n",
    "    with open(csv_file, 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "        for line in txt_file:\n",
    "            if not all(ord(c) < 128 for c in line):\n",
    "                line = unidecode.unidecode(line)\n",
    "            row = line.rstrip('\\t\\r\\n').split('\\t')\n",
    "            if len(row) != 4:\n",
    "                print('skipping bad row (length %s, expected 4):' % len(row))\n",
    "                print(row)\n",
    "                continue\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "            \n",
    "txt_file = 'test.tsv'\n",
    "csv_file =  'test.csv'\n",
    "\n",
    "with open(txt_file, 'r') as txt_file:\n",
    "    with open(csv_file, 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "        for line in txt_file:\n",
    "            if not all(ord(c) < 128 for c in line):\n",
    "                line = unidecode.unidecode(line)\n",
    "            row = line.rstrip('\\t\\r\\n').split('\\t')\n",
    "            if len(row) != 3:\n",
    "                print('skipping bad row (length %s, expected 4):' % len(row))\n",
    "                print(row)\n",
    "                continue\n",
    "            csv_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names from created csv files\n",
    "in_file_nm = 'train.csv'\n",
    "in_file = open(in_file_nm, 'r')\n",
    "columns = in_file.readline().strip('\\n').split(',')\n",
    "print('train.csv columns ', columns)\n",
    "in_file_nm = 'test.csv'\n",
    "in_file = open(in_file_nm, 'r')\n",
    "columns = in_file.readline().strip('\\n').split(',')\n",
    "print('test.csv columns ', columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  load data to a Postgres db in docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view pg environment\n",
    "try:\n",
    "    connection = psycopg2.connect(user = user,\n",
    "                                  password = password,\n",
    "                                  host = host,\n",
    "                                  port = port,\n",
    "                                  database = database)\n",
    "    cursor = connection.cursor()\n",
    "    # Print PostgreSQL Connection properties\n",
    "    print ( connection.get_dsn_parameters(),\"\\n\")\n",
    "    # Print PostgreSQL version\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"You are connected to - \", record,\"\\n\")\n",
    "except (Exception, psycopg2.Error) as error :\n",
    "    print (\"Error while connecting to PostgreSQL\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create table sql queries\n",
    "#\"\"', '\"\"', '\"\"', '\"\"\n",
    "sent_movie_train = '''\n",
    "CREATE TABLE sent_movie_train\n",
    "(\n",
    " PhraseId\tINTEGER PRIMARY KEY,\n",
    " SentenceId\tINTEGER,\n",
    " Phrase\t TEXT,\n",
    " Sentiment INTEGER   \n",
    " )\n",
    "'''\n",
    "sent_movie_test = '''\n",
    "CREATE TABLE sent_movie_test\n",
    "(\n",
    " PhraseId\tINTEGER PRIMARY KEY,\n",
    " SentenceId\tINTEGER,\n",
    " Phrase\t TEXT\n",
    " )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute table table creations\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(user = user,\n",
    "                                  password = password,\n",
    "                                  host = host,\n",
    "                                  port = port,\n",
    "                                  database = database)\n",
    "    cursor = connection.cursor()\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    engine_string = \"postgresql://{}:{}@{}:{}/{}\".format(user, password, host, port, database)\n",
    "    engine = create_engine(engine_string)\n",
    "    \n",
    "       \n",
    "    cursor.execute(sent_movie_train)\n",
    "    print('created table ', sent_movie_train)\n",
    "\n",
    "    cursor.execute(sent_movie_test)\n",
    "    print('created table ', sent_movie_test)\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while creating PostgreSQL table\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good explaination of how top copy table https://www.laurivan.com/load-a-csv-file-with-header-in-postgres-via-psycopg/\n",
    "\n",
    "SQL_STATEMENT = \"\"\"\n",
    "    COPY %s FROM STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process files for created tables.\n",
    "def process_file(conn, table_name, file_object):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.copy_expert(sql=SQL_STATEMENT % table_name, file=file_object)\n",
    "    conn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save processed csv files to pg db\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(user = user,\n",
    "                                  password = password,\n",
    "                                  host = host,\n",
    "                                  port = port,\n",
    "                                  database = database)\n",
    "    cursor = connection.cursor()\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    engine_string = \"postgresql://{}:{}@{}:{}/{}\".format(user, password, host, port, database)\n",
    "    engine = create_engine(engine_string)\n",
    "    \n",
    "    file = open(\"train.csv\")\n",
    "    process_file(connection, 'sent_movie_train', file)\n",
    "    print('populated table sent_movie_train')\n",
    "    \n",
    "    file = open(\"test.csv\")\n",
    "    process_file(connection, 'sent_movie_test', file)\n",
    "    print('populated table sent_movie_test')   \n",
    "    \n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while creating PostgreSQL table\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  clean train and test tables to adapt Kaggle dataset to reflect more likely original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean train and test tables to show one sentence instead per review(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table of full sentences \n",
    "sql = '''\n",
    "CREATE TABLE sent_train_tble AS\n",
    "SELECT\n",
    "s.phraseid,\n",
    "s.sentenceid,\n",
    "s.phrase,\n",
    "s.sentiment\n",
    "FROM(SELECT\n",
    "  p.phraseid,\n",
    "  p.sentenceid,\n",
    "  p.phrase,\n",
    "  p.sentiment,\n",
    "  ROW_NUMBER() OVER(PARTITION BY p.sentenceid ORDER BY p.phraseid ASC) AS phraseid_rk\n",
    "  FROM sent_movie_train p\n",
    "  GROUP BY sentenceid, phraseid) s\n",
    "where s.phraseid_rk = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(user = user,\n",
    "                                  password = password,\n",
    "                                  host = host,\n",
    "                                  port = port,\n",
    "                                  database = database)\n",
    "    cursor = connection.cursor()\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    engine_string = \"postgresql://{}:{}@{}:{}/{}\".format(user, password, host, port, database)\n",
    "    engine = create_engine(engine_string)\n",
    "    \n",
    "       \n",
    "    cursor.execute(sql)\n",
    "    print('created table sent_train_tble', sql)\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while creating PostgreSQL table\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table of full sentences \n",
    "sql = '''\n",
    "CREATE TABLE sent_test_tble AS\n",
    "SELECT\n",
    "s.phraseid,\n",
    "s.sentenceid,\n",
    "s.phrase\n",
    "FROM(SELECT\n",
    "  p.phraseid,\n",
    "  p.sentenceid,\n",
    "  p.phrase,\n",
    "  ROW_NUMBER() OVER(PARTITION BY p.sentenceid ORDER BY p.phraseid ASC) AS phraseid_rk\n",
    "  FROM sent_movie_test p\n",
    "  GROUP BY sentenceid, phraseid) s\n",
    "where s.phraseid_rk = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(user = user,\n",
    "                                  password = password,\n",
    "                                  host = host,\n",
    "                                  port = port,\n",
    "                                  database = database)\n",
    "    cursor = connection.cursor()\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    engine_string = \"postgresql://{}:{}@{}:{}/{}\".format(user, password, host, port, database)\n",
    "    engine = create_engine(engine_string)\n",
    "    \n",
    "       \n",
    "    cursor.execute(sql)\n",
    "    print('created table sent_train_tble', sql)\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while creating PostgreSQL table\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
